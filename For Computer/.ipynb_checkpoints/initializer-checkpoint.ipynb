{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfe7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def initialize():\n",
    "    # importing the dataset\n",
    "    with open('content.json') as content:\n",
    "        data = json.load(content)\n",
    "    # getting all the data to lists\n",
    "    tags = []\n",
    "    inputs = []\n",
    "    responses = {}\n",
    "    for intent in data['intents']:\n",
    "        responses[intent['tag']] = intent['responses']\n",
    "        for lines in intent['input']:\n",
    "            inputs.append(lines)\n",
    "            tags.append(intent['tag'])\n",
    "                \n",
    "    # converting to dataframe\n",
    "    data = pd.DataFrame({\"inputs\":inputs,\"tags\":tags})\n",
    "    # Preprocessing the data\n",
    "    # convertin words to lowercase and removing the punctuation\n",
    "    data['inputs'] = data['inputs'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])\n",
    "    data['inputs'] = data['inputs'].apply(lambda wrd: ''.join(wrd))\n",
    "    data\n",
    "    \n",
    "    # tokenize the data\n",
    "    tokenizer = Tokenizer(\n",
    "        # num_words=None,\n",
    "        num_words=200000,\n",
    "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "        lower=True,\n",
    "        split=' ',\n",
    "        char_level=False,\n",
    "        oov_token=\"<OOV>\", #OOV means OOV\n",
    "        analyzer=None\n",
    "    )\n",
    "    tokenizer.fit_on_texts(data['inputs'])\n",
    "    train = tokenizer.texts_to_sequences(data['inputs'])\n",
    "\n",
    "    #apply padding\n",
    "    x_train = pad_sequences(train)\n",
    "\n",
    "    # encode the outputs\n",
    "    labelEncoder = LabelEncoder()\n",
    "    y_train = labelEncoder.fit_transform(data['tags'])\n",
    "    \n",
    "    return tokenizer,labelEncoder,responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbccfae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
